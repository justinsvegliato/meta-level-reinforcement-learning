{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.0 (SDL 2.0.16, Python 3.8.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib confmisc.c:767:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_card_driver returned error: No such file or directory\n",
      "ALSA lib confmisc.c:392:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1246:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5220:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2642:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "ALSA lib confmisc.c:767:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_card_driver returned error: No such file or directory\n",
      "ALSA lib confmisc.c:392:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1246:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5220:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2642:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "ALSA lib confmisc.c:767:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_card_driver returned error: No such file or directory\n",
      "ALSA lib confmisc.c:392:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1246:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5220:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2642:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "ALSA lib confmisc.c:767:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_card_driver returned error: No such file or directory\n",
      "ALSA lib confmisc.c:392:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1246:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5220:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2642:(snd_pcm_open_noupdate) Unknown PCM default\n"
     ]
    }
   ],
   "source": [
    "from mlrl.experiments.train_maze_agent import (\n",
    "    create_maze_meta_env, RestrictedActionsMazeState, create_batched_tf_meta_env, \n",
    "    create_agent, create_training_run, get_maze_name\n",
    ")\n",
    "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
    "from tf_agents.environments.gym_wrapper import GymWrapper\n",
    "from tf_agents.environments.batched_py_environment import BatchedPyEnvironment\n",
    "from tf_agents.train.utils import spec_utils\n",
    "\n",
    "args = {\n",
    "    'agent': 'ppo_agent', \n",
    "    'meta_time_limit': 500,\n",
    "}\n",
    "env_batch_size = 2\n",
    "\n",
    "env = BatchedPyEnvironment([\n",
    "    GymWrapper(create_maze_meta_env(RestrictedActionsMazeState, args)) \n",
    "    for _ in range(env_batch_size)\n",
    "], multithreading=False)\n",
    "\n",
    "eval_env = BatchedPyEnvironment([\n",
    "    GymWrapper(create_maze_meta_env(RestrictedActionsMazeState, args)) \n",
    "    for _ in range(env_batch_size)\n",
    "], multithreading=False)\n",
    "\n",
    "env.reset()\n",
    "\n",
    "observation_tensor_spec, action_tensor_spec, time_step_tensor_spec = (\n",
    "      spec_utils.get_tensor_specs(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-04 23:05:10.498104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-04 23:05:10.548942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-04 23:05:10.551386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-04 23:05:10.552682: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-04 23:05:10.562433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-04 23:05:10.564604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-04 23:05:10.566649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-04 23:05:14.295833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-04 23:05:14.296952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-04 23:05:14.296977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-10-04 23:05:14.298067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-04 23:05:14.298137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2111 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.agents.ppo.ppo_agent import PPOAgent\n",
    "from tf_agents.train.utils import train_utils\n",
    "from tf_agents.networks.mask_splitter_network import MaskSplitterNetwork\n",
    "\n",
    "from mlrl.meta.meta_env import mask_token_splitter\n",
    "from mlrl.meta.search_networks import create_action_distribution_network\n",
    "from mlrl.meta.search_networks import create_value_network\n",
    "\n",
    "train_step = train_utils.create_train_step()\n",
    "\n",
    "network_kwargs = {\n",
    "    'n_heads': 3,\n",
    "    'n_layers': 2,\n",
    "    'head_dim': 32,\n",
    "}\n",
    "\n",
    "value_net=create_value_network(observation_tensor_spec, **network_kwargs)\n",
    "\n",
    "actor_net = create_action_distribution_network(observation_tensor_spec['search_tree_tokens'],\n",
    "                                               action_tensor_spec,\n",
    "                                               **network_kwargs)\n",
    "\n",
    "masked_actor_net = MaskSplitterNetwork(mask_token_splitter,\n",
    "                                       actor_net,\n",
    "                                       input_tensor_spec=observation_tensor_spec,\n",
    "                                       passthrough_mask=True)\n",
    "\n",
    "\n",
    "agent = PPOAgent(\n",
    "    time_step_tensor_spec,\n",
    "    action_tensor_spec,\n",
    "    actor_net=masked_actor_net,\n",
    "    value_net=value_net,\n",
    "    optimizer=tf.keras.optimizers.Adam(3e-4),\n",
    "    train_step_counter=train_step,\n",
    "    compute_value_and_advantage_in_train=False,\n",
    "    update_normalizers_in_train=False,\n",
    "    normalize_observations=False,\n",
    "    discount_factor=0.99,\n",
    "    num_epochs=1,  # deprecated param\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=env.batch_size or 1,\n",
    "    max_length=10000\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess_seq(experience, info):\n",
    "    return agent.preprocess_sequence(experience), info\n",
    "\n",
    "\n",
    "def dataset_fn():\n",
    "    ds = replay_buffer.as_dataset(sample_batch_size=4, num_steps=64)\n",
    "    return ds.map(preprocess_seq).prefetch(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.train import actor\n",
    "from tf_agents.train import learner\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.train import triggers\n",
    "\n",
    "import os\n",
    "\n",
    "root_dir = './runs/ppo_agent/test'\n",
    "\n",
    "summary_interval = 1000\n",
    "collect_sequence_length = 2048\n",
    "policy_save_interval = 5000\n",
    "\n",
    "\n",
    "saved_model_dir = os.path.join(root_dir, learner.POLICY_SAVED_MODEL_DIR)\n",
    "collect_env_step_metric = py_metrics.EnvironmentSteps()\n",
    "learning_triggers = [\n",
    "    triggers.PolicySavedModelTrigger(\n",
    "        saved_model_dir,\n",
    "        agent,\n",
    "        train_step,\n",
    "        interval=policy_save_interval,\n",
    "        metadata_metrics={\n",
    "            triggers.ENV_STEP_METADATA_KEY: collect_env_step_metric\n",
    "        }),\n",
    "    triggers.StepPerSecondLogTrigger(train_step, interval=summary_interval),\n",
    "]\n",
    "\n",
    "collect_actor = actor.Actor(\n",
    "    env,\n",
    "    agent.collect_policy,\n",
    "    train_step,\n",
    "    steps_per_run=collect_sequence_length,\n",
    "    observers=[replay_buffer.add_batch],\n",
    "    metrics=actor.collect_metrics(buffer_size=collect_sequence_length),\n",
    "    reference_metrics=[collect_env_step_metric],\n",
    "    summary_dir=os.path.join(root_dir, learner.TRAIN_DIR),\n",
    "    summary_interval=summary_interval)\n",
    "\n",
    "collect_actor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.policies import py_tf_eager_policy\n",
    "\n",
    "eval_steps=1000\n",
    "\n",
    "eval_greedy_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
    "    agent.policy, use_tf_function=True)\n",
    "\n",
    "eval_actor = actor.Actor(\n",
    "    eval_env,\n",
    "    eval_greedy_policy,\n",
    "    train_step,\n",
    "    metrics=actor.eval_metrics(buffer_size=10),\n",
    "    reference_metrics=[collect_env_step_metric],\n",
    "    summary_dir=os.path.join(root_dir, 'eval'),\n",
    "    steps_per_run=eval_steps)\n",
    "\n",
    "# eval_actor.run_and_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:458: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:458: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.train.ppo_learner import PPOLearner\n",
    "\n",
    "ppo_learner = PPOLearner(\n",
    "    root_dir,\n",
    "    train_step,\n",
    "    agent,\n",
    "    experience_dataset_fn=dataset_fn,\n",
    "    normalization_dataset_fn=dataset_fn,\n",
    "    num_samples=1, num_epochs=20,  # num samples * num epochs = num iterations per run call\n",
    "    triggers=learning_triggers,\n",
    "    shuffle_buffer_size=collect_sequence_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.3 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/drcope/mlrl/runs/2jwumgpa\" target=\"_blank\">dulcet-universe-64</a></strong> to <a href=\"https://wandb.ai/drcope/mlrl\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Collect stats:\n",
      "NumberOfEpisodes: 32.000, EnvironmentSteps: 2048.000, AverageReturn: 0.955, AverageEpisodeLength: 62.938\n",
      "Training info:\n",
      "Loss: 0.92742, KL Penalty Loss: 0.01046, Entropy: 0.00000, Value Estimation Loss: 0.95825, PG Loss -0.04129\n",
      "Evaluation stats:\n",
      "AverageReturn: 0.953, AverageEpisodeLength: 80.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (1080, 360) to (1088, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 1\n",
      "Collect stats:\n",
      "NumberOfEpisodes: 30.000, EnvironmentSteps: 2048.000, AverageReturn: 0.946, AverageEpisodeLength: 66.700\n",
      "Training info:\n",
      "Loss: 0.47094, KL Penalty Loss: 0.00043, Entropy: 0.00000, Value Estimation Loss: 0.54559, PG Loss -0.07508\n",
      "\n",
      "Iteration: 2\n",
      "Collect stats:\n",
      "NumberOfEpisodes: 30.000, EnvironmentSteps: 2048.000, AverageReturn: 0.953, AverageEpisodeLength: 65.600\n",
      "Training info:\n",
      "Loss: 0.10185, KL Penalty Loss: 0.00122, Entropy: 0.00000, Value Estimation Loss: 0.16324, PG Loss -0.06261\n",
      "\n",
      "Iteration: 3\n",
      "Collect stats:\n",
      "NumberOfEpisodes: 34.000, EnvironmentSteps: 2048.000, AverageReturn: 0.956, AverageEpisodeLength: 59.765\n",
      "Training info:\n",
      "Loss: 0.11847, KL Penalty Loss: 0.00749, Entropy: 0.00000, Value Estimation Loss: 0.21133, PG Loss -0.10034\n",
      "\n",
      "Iteration: 4\n",
      "Collect stats:\n",
      "NumberOfEpisodes: 31.000, EnvironmentSteps: 2049.000, AverageReturn: 0.954, AverageEpisodeLength: 63.581\n",
      "Training info:\n",
      "Loss: -0.00721, KL Penalty Loss: 0.00319, Entropy: 0.00000, Value Estimation Loss: 0.09118, PG Loss -0.10158\n",
      "\n",
      "Iteration: 5\n",
      "Collect stats:\n",
      "NumberOfEpisodes: 33.000, EnvironmentSteps: 2049.000, AverageReturn: 0.959, AverageEpisodeLength: 59.818\n",
      "Training info:\n",
      "Loss: -0.01085, KL Penalty Loss: 0.00116, Entropy: 0.00000, Value Estimation Loss: 0.13997, PG Loss -0.15198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (1080, 360) to (1088, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation stats:\n",
      "AverageReturn: 0.953, AverageEpisodeLength: 80.000\n",
      "\n",
      "Iteration: 6\n",
      "Collect stats:\n",
      "NumberOfEpisodes: 34.000, EnvironmentSteps: 2048.000, AverageReturn: 0.962, AverageEpisodeLength: 57.559\n",
      "Training info:\n",
      "Loss: 0.03863, KL Penalty Loss: 0.00297, Entropy: 0.00000, Value Estimation Loss: 0.14078, PG Loss -0.10512\n",
      "\n",
      "Iteration: 7\n",
      "Collect stats:\n",
      "NumberOfEpisodes: 33.000, EnvironmentSteps: 2049.000, AverageReturn: 0.960, AverageEpisodeLength: 58.879\n",
      "Training info:\n",
      "Loss: 0.01563, KL Penalty Loss: 0.00520, Entropy: 0.00000, Value Estimation Loss: 0.14271, PG Loss -0.13228\n",
      "\n",
      "Iteration: 8\n",
      "Collect stats:\n",
      "NumberOfEpisodes: 32.000, EnvironmentSteps: 2048.000, AverageReturn: 0.959, AverageEpisodeLength: 62.469\n",
      "Training info:\n",
      "Loss: 0.03292, KL Penalty Loss: 0.00018, Entropy: 0.00000, Value Estimation Loss: 0.15136, PG Loss -0.11862\n",
      "\n",
      "Iteration: 9\n",
      "Collect stats:\n",
      "NumberOfEpisodes: 35.000, EnvironmentSteps: 2049.000, AverageReturn: 0.969, AverageEpisodeLength: 55.400\n",
      "Training info:\n",
      "Loss: -0.02190, KL Penalty Loss: 0.00144, Entropy: 0.00000, Value Estimation Loss: 0.19485, PG Loss -0.21819\n",
      "\n",
      "Iteration: 10\n",
      "Collect stats:\n",
      "NumberOfEpisodes: 34.000, EnvironmentSteps: 2048.000, AverageReturn: 0.957, AverageEpisodeLength: 58.706\n",
      "Training info:\n",
      "Loss: 0.09258, KL Penalty Loss: 0.00000, Entropy: 0.00000, Value Estimation Loss: 0.22168, PG Loss -0.12909\n",
      "Evaluation stats:\n",
      "AverageReturn: 0.953, AverageEpisodeLength: 80.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (1080, 360) to (1088, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 11\n",
      "Collect stats:\n",
      "NumberOfEpisodes: 37.000, EnvironmentSteps: 2048.000, AverageReturn: 0.959, AverageEpisodeLength: 54.541\n",
      "Training info:\n",
      "Loss: 0.02425, KL Penalty Loss: 0.00001, Entropy: 0.00000, Value Estimation Loss: 0.13519, PG Loss -0.11095\n",
      "\n",
      "Iteration: 12\n",
      "Collect stats:\n",
      "NumberOfEpisodes: 42.000, EnvironmentSteps: 2049.000, AverageReturn: 0.975, AverageEpisodeLength: 47.095\n",
      "Training info:\n",
      "Loss: 0.03540, KL Penalty Loss: 0.00062, Entropy: 0.00000, Value Estimation Loss: 0.17071, PG Loss -0.13593\n",
      "\n",
      "Iteration: 13\n",
      "Collect stats:\n",
      "NumberOfEpisodes: 43.000, EnvironmentSteps: 2049.000, AverageReturn: 0.972, AverageEpisodeLength: 46.605\n",
      "Training info:\n",
      "Loss: 0.19583, KL Penalty Loss: 0.00498, Entropy: 0.00000, Value Estimation Loss: 0.27726, PG Loss -0.08642\n",
      "\n",
      "Iteration: 14\n",
      "Collect stats:\n",
      "NumberOfEpisodes: 43.000, EnvironmentSteps: 2049.000, AverageReturn: 0.972, AverageEpisodeLength: 45.558\n",
      "Training info:\n",
      "Loss: -0.01204, KL Penalty Loss: 0.00729, Entropy: 0.00000, Value Estimation Loss: 0.12330, PG Loss -0.14263\n",
      "\n",
      "Iteration: 15\n",
      "Collect stats:\n",
      "NumberOfEpisodes: 48.000, EnvironmentSteps: 2048.000, AverageReturn: 0.977, AverageEpisodeLength: 41.062\n",
      "Training info:\n",
      "Loss: -0.03439, KL Penalty Loss: 0.00000, Entropy: 0.00000, Value Estimation Loss: 0.11722, PG Loss -0.15161\n",
      "Evaluation stats:\n",
      "AverageReturn: 0.971, AverageEpisodeLength: 62.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (1080, 360) to (1088, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 16\n",
      "Collect stats:\n",
      "NumberOfEpisodes: 49.000, EnvironmentSteps: 2049.000, AverageReturn: 0.982, AverageEpisodeLength: 41.592\n",
      "Training info:\n",
      "Loss: 0.08427, KL Penalty Loss: 0.00109, Entropy: 0.00000, Value Estimation Loss: 0.16816, PG Loss -0.08498\n",
      "\n",
      "Iteration: 17\n",
      "Collect stats:\n",
      "NumberOfEpisodes: 49.000, EnvironmentSteps: 2049.000, AverageReturn: 0.982, AverageEpisodeLength: 40.816\n",
      "Training info:\n",
      "Loss: -0.10123, KL Penalty Loss: 0.00000, Entropy: 0.00000, Value Estimation Loss: 0.07922, PG Loss -0.18046\n",
      "\n",
      "Iteration: 18\n",
      "Collect stats:\n",
      "NumberOfEpisodes: 54.000, EnvironmentSteps: 2048.000, AverageReturn: 0.987, AverageEpisodeLength: 37.259\n",
      "Training info:\n",
      "Loss: -0.09559, KL Penalty Loss: 0.01591, Entropy: 0.00000, Value Estimation Loss: 0.07045, PG Loss -0.18195\n",
      "\n",
      "Iteration: 19\n",
      "Collect stats:\n",
      "NumberOfEpisodes: 56.000, EnvironmentSteps: 2048.000, AverageReturn: 0.982, AverageEpisodeLength: 36.089\n",
      "Training info:\n",
      "Loss: 0.00144, KL Penalty Loss: 0.00000, Entropy: 0.00000, Value Estimation Loss: 0.08732, PG Loss -0.08588\n",
      "\n",
      "Iteration: 20\n",
      "Collect stats:\n",
      "NumberOfEpisodes: 57.000, EnvironmentSteps: 2049.000, AverageReturn: 0.990, AverageEpisodeLength: 34.491\n",
      "Training info:\n",
      "Loss: -0.05089, KL Penalty Loss: 0.00000, Entropy: 0.00000, Value Estimation Loss: 0.08216, PG Loss -0.13305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (1080, 360) to (1088, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation stats:\n",
      "AverageReturn: 0.996, AverageEpisodeLength: 37.000\n",
      "\n",
      "Iteration: 21\n",
      "Collect stats:\n",
      "NumberOfEpisodes: 57.000, EnvironmentSteps: 2049.000, AverageReturn: 0.992, AverageEpisodeLength: 34.807\n",
      "Training info:\n",
      "Loss: -0.02823, KL Penalty Loss: 0.00000, Entropy: 0.00000, Value Estimation Loss: 0.09493, PG Loss -0.12315\n",
      "\n",
      "Iteration: 22\n",
      "Collect stats:\n",
      "NumberOfEpisodes: 68.000, EnvironmentSteps: 2048.000, AverageReturn: 0.997, AverageEpisodeLength: 29.838\n",
      "Training info:\n",
      "Loss: -0.03970, KL Penalty Loss: 0.00193, Entropy: 0.00000, Value Estimation Loss: 0.10951, PG Loss -0.15115\n",
      "\n",
      "Iteration: 23\n",
      "Collect stats:\n",
      "NumberOfEpisodes: 76.000, EnvironmentSteps: 2048.000, AverageReturn: 0.998, AverageEpisodeLength: 26.408\n",
      "Training info:\n",
      "Loss: -0.06981, KL Penalty Loss: 0.01519, Entropy: 0.00000, Value Estimation Loss: 0.08551, PG Loss -0.17051\n",
      "\n",
      "Iteration: 24\n",
      "Collect stats:\n",
      "NumberOfEpisodes: 93.000, EnvironmentSteps: 2049.000, AverageReturn: 1.003, AverageEpisodeLength: 21.699\n",
      "Training info:\n",
      "Loss: -0.09553, KL Penalty Loss: 0.00134, Entropy: 0.00000, Value Estimation Loss: 0.06442, PG Loss -0.16130\n",
      "\n",
      "Iteration: 25\n",
      "Collect stats:\n",
      "NumberOfEpisodes: 94.000, EnvironmentSteps: 2049.000, AverageReturn: 1.003, AverageEpisodeLength: 21.734\n",
      "Training info:\n",
      "Loss: -0.06031, KL Penalty Loss: 0.00000, Entropy: 0.00000, Value Estimation Loss: 0.07092, PG Loss -0.13124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (1080, 360) to (1088, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation stats:\n",
      "AverageReturn: 1.022, AverageEpisodeLength: 11.000\n",
      "\n",
      "Iteration: 26\n",
      "Waiting for debugger attach\n",
      "Debugger attached\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 165... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5145acfb4e1a468baddd605c9d77db2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 2.56MB of 2.56MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AverageEpisodeLength</td><td>▇██▇█▇▇▇▇▆▇▆▅▅▅▄▄▄▃▃▃▃▂▂▁▁</td></tr><tr><td>AverageReturn</td><td>▂▁▂▂▂▃▃▃▃▄▂▃▅▄▄▅▅▅▆▅▆▇▇▇██</td></tr><tr><td>EnvironmentSteps</td><td>▁▁▁▁██▁█▁█▁▁███▁██▁▁██▁▁██</td></tr><tr><td>EvalAverageEpisodeLength</td><td>███▆▄▁</td></tr><tr><td>EvalAverageReturn</td><td>▁▁▁▃▅█</td></tr><tr><td>NumberOfEpisodes</td><td>▁▁▁▁▁▁▁▁▁▂▁▂▂▂▂▃▃▃▄▄▄▄▅▆██</td></tr><tr><td>entropy_regularization_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>iteration</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>kl_penalty_loss</td><td>▆▁▂▄▂▂▂▃▁▂▁▁▁▃▄▁▁▁█▁▁▁▂█▂▁</td></tr><tr><td>l2_regularization_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▅▂▂▂▂▂▂▂▂▂▂▂▃▂▁▂▁▁▂▁▁▁▁▁▁</td></tr><tr><td>policy_gradient_loss</td><td>█▇▇▆▆▄▅▄▅▁▅▅▄▆▄▄▆▂▂▆▄▅▄▃▃▄</td></tr><tr><td>value_estimation_loss</td><td>█▅▂▂▁▂▂▂▂▂▂▂▂▃▁▁▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AverageEpisodeLength</td><td>21.73404</td></tr><tr><td>AverageReturn</td><td>1.00293</td></tr><tr><td>EnvironmentSteps</td><td>2049</td></tr><tr><td>EvalAverageEpisodeLength</td><td>11.0</td></tr><tr><td>EvalAverageReturn</td><td>1.02199</td></tr><tr><td>NumberOfEpisodes</td><td>94</td></tr><tr><td>entropy_regularization_loss</td><td>0.0</td></tr><tr><td>iteration</td><td>25</td></tr><tr><td>kl_penalty_loss</td><td>0.0</td></tr><tr><td>l2_regularization_loss</td><td>0.0</td></tr><tr><td>loss</td><td>-0.06031</td></tr><tr><td>policy_gradient_loss</td><td>-0.13124</td></tr><tr><td>value_estimation_loss</td><td>0.07092</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 6 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dulcet-universe-64</strong>: <a href=\"https://wandb.ai/drcope/mlrl/runs/2jwumgpa\" target=\"_blank\">https://wandb.ai/drcope/mlrl/runs/2jwumgpa</a><br/>\n",
       "Find logs at: <code>./wandb/run-20221004_230828-2jwumgpa/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/dylancope/projects/mlrl/PPO Training.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/dylancope/projects/mlrl/PPO%20Training.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mfor\u001b[39;00m metric \u001b[39min\u001b[39;00m collect_actor\u001b[39m.\u001b[39mmetrics:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/dylancope/projects/mlrl/PPO%20Training.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m       metric\u001b[39m.\u001b[39mreset()\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/dylancope/projects/mlrl/PPO%20Training.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m collect_actor\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/dylancope/projects/mlrl/PPO%20Training.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mCollect stats:\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/dylancope/projects/mlrl/PPO%20Training.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmetric\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mmetric\u001b[39m.\u001b[39mresult()\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39mfor\u001b[39;00m metric \u001b[39min\u001b[39;00m collect_actor\u001b[39m.\u001b[39mmetrics]))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tf_agents/train/actor.py:147\u001b[0m, in \u001b[0;36mActor.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 147\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time_step, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_driver\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m    148\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_time_step, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_policy_state)\n\u001b[1;32m    150\u001b[0m   \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_write_summaries \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_summary_interval \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    151\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_summary \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_summary_interval):\n\u001b[1;32m    152\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite_metric_summaries()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tf_agents/drivers/py_driver.py:111\u001b[0m, in \u001b[0;36mPyDriver.run\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m    108\u001b[0m   policy_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mget_initial_state(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mbatch_size \u001b[39mor\u001b[39;00m \u001b[39m1\u001b[39m)\n\u001b[1;32m    110\u001b[0m action_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39maction(time_step, policy_state)\n\u001b[0;32m--> 111\u001b[0m next_time_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action_step\u001b[39m.\u001b[39;49maction)\n\u001b[1;32m    113\u001b[0m \u001b[39m# When using observer (for the purpose of training), only the previous\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39m# policy_state is useful. Therefore substitube it in the PolicyStep and\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[39m# consume it w/ the observer.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m action_step_with_previous_state \u001b[39m=\u001b[39m action_step\u001b[39m.\u001b[39m_replace(state\u001b[39m=\u001b[39mpolicy_state)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tf_agents/environments/py_environment.py:232\u001b[0m, in \u001b[0;36mPyEnvironment.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_time_step \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshould_reset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_time_step)):\n\u001b[1;32m    230\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset()\n\u001b[0;32m--> 232\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_time_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step(action)\n\u001b[1;32m    233\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_time_step\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tf_agents/environments/batched_py_environment.py:173\u001b[0m, in \u001b[0;36mBatchedPyEnvironment._step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(unstacked_actions) \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size:\n\u001b[1;32m    170\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    171\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mPrimary dimension of action items does not match \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mbatch size: \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m vs. \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mlen\u001b[39m(unstacked_actions), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size))\n\u001b[0;32m--> 173\u001b[0m time_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute(\n\u001b[1;32m    174\u001b[0m     \u001b[39mlambda\u001b[39;49;00m env_action: env_action[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mstep(env_action[\u001b[39m1\u001b[39;49m]),\n\u001b[1;32m    175\u001b[0m     \u001b[39mzip\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_envs, unstacked_actions))\n\u001b[1;32m    176\u001b[0m \u001b[39mreturn\u001b[39;00m nest_utils\u001b[39m.\u001b[39mstack_nested_arrays(time_steps)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tf_agents/environments/batched_py_environment.py:108\u001b[0m, in \u001b[0;36mBatchedPyEnvironment._execute\u001b[0;34m(self, fn, iterable)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool\u001b[39m.\u001b[39mmap(fn, iterable)\n\u001b[1;32m    107\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 108\u001b[0m   \u001b[39mreturn\u001b[39;00m [fn(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m iterable]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tf_agents/environments/batched_py_environment.py:108\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool\u001b[39m.\u001b[39mmap(fn, iterable)\n\u001b[1;32m    107\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 108\u001b[0m   \u001b[39mreturn\u001b[39;00m [fn(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m iterable]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tf_agents/environments/batched_py_environment.py:174\u001b[0m, in \u001b[0;36mBatchedPyEnvironment._step.<locals>.<lambda>\u001b[0;34m(env_action)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(unstacked_actions) \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size:\n\u001b[1;32m    170\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    171\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mPrimary dimension of action items does not match \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mbatch size: \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m vs. \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mlen\u001b[39m(unstacked_actions), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size))\n\u001b[1;32m    173\u001b[0m time_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_execute(\n\u001b[0;32m--> 174\u001b[0m     \u001b[39mlambda\u001b[39;00m env_action: env_action[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mstep(env_action[\u001b[39m1\u001b[39;49m]),\n\u001b[1;32m    175\u001b[0m     \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_envs, unstacked_actions))\n\u001b[1;32m    176\u001b[0m \u001b[39mreturn\u001b[39;00m nest_utils\u001b[39m.\u001b[39mstack_nested_arrays(time_steps)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tf_agents/environments/py_environment.py:232\u001b[0m, in \u001b[0;36mPyEnvironment.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_time_step \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshould_reset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_time_step)):\n\u001b[1;32m    230\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset()\n\u001b[0;32m--> 232\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_time_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step(action)\n\u001b[1;32m    233\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_time_step\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tf_agents/environments/gym_wrapper.py:216\u001b[0m, in \u001b[0;36mGymWrapper._step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    212\u001b[0m   action \u001b[39m=\u001b[39m action\u001b[39m.\u001b[39mitem()\n\u001b[1;32m    214\u001b[0m \u001b[39m# TODO(oars): Figure out how tuple or dict actions will be generated by the\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[39m# agents and if we can pass them through directly to gym.\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m observation, reward, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_done, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gym_env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    218\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match_obs_space_dtype:\n\u001b[1;32m    219\u001b[0m   observation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_to_obs_space_dtype(observation)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/wrappers/time_limit.py:16\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     15\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling reset()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 16\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     17\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     18\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m/tf/mlrl/meta/meta_env.py:339\u001b[0m, in \u001b[0;36mMetaEnv.step\u001b[0;34m(self, computational_action)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    338\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dump_debug_info(computational_action, e)\n\u001b[0;32m--> 339\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/tf/mlrl/meta/meta_env.py:324\u001b[0m, in \u001b[0;36mMetaEnv.step\u001b[0;34m(self, computational_action)\u001b[0m\n\u001b[1;32m    321\u001b[0m node_idx \u001b[39m=\u001b[39m (computational_action \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_object_actions\n\u001b[1;32m    322\u001b[0m object_action \u001b[39m=\u001b[39m (computational_action \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_object_actions\n\u001b[0;32m--> 324\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree\u001b[39m.\u001b[39;49mexpand(node_idx, object_action)\n\u001b[1;32m    325\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_computations \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    327\u001b[0m \u001b[39m# Compute reward (named \"last_meta_reward\" for readability in later access)\u001b[39;00m\n",
      "File \u001b[0;32m/tf/mlrl/meta/search_tree.py:249\u001b[0m, in \u001b[0;36mSearchTree.expand\u001b[0;34m(self, node_idx, action)\u001b[0m\n\u001b[1;32m    247\u001b[0m node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_list[node_idx]\n\u001b[1;32m    248\u001b[0m \u001b[39mif\u001b[39;00m node\u001b[39m.\u001b[39mcan_expand():\n\u001b[0;32m--> 249\u001b[0m     child_node \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39;49mexpand_node(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, action, \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode_list))\n\u001b[1;32m    250\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_list\u001b[39m.\u001b[39mappend(child_node)\n\u001b[1;32m    251\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/tf/mlrl/meta/search_tree.py:138\u001b[0m, in \u001b[0;36mSearchTreeNode.expand_node\u001b[0;34m(self, env, action_idx, new_node_id)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mCannot expand a terminal node\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    137\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mset_environment_to_state(env)\n\u001b[0;32m--> 138\u001b[0m object_action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate\u001b[39m.\u001b[39;49mget_actions()[action_idx]\n\u001b[1;32m    139\u001b[0m _, reward, done, \u001b[39m*\u001b[39m_ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(object_action)\n\u001b[1;32m    140\u001b[0m next_state: ObjectState \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mextract_state(env)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from mlrl.utils.render_utils import create_policy_eval_video\n",
    "\n",
    "\n",
    "videos_dir = root_dir + '/videos'\n",
    "\n",
    "from pathlib import Path\n",
    "Path(videos_dir).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "\n",
    "eval_interval = 5\n",
    "num_iterations = 100\n",
    "\n",
    "try:\n",
    "      config = {\n",
    "            'name': '5x5_maze_ppo',\n",
    "            'eval_interval': eval_interval,\n",
    "            'num_iterations': num_iterations,\n",
    "            'summary_interval': summary_interval,\n",
    "            'collect_sequence_length': collect_sequence_length,\n",
    "            'policy_save_interval': policy_save_interval,\n",
    "            'num_samples': ppo_learner._num_samples,\n",
    "            'num_epochs': ppo_learner._num_epochs,\n",
    "            'meta_discount_factor': agent._discount_factor,\n",
    "            'max_tree_size': env.envs[0].max_tree_size,\n",
    "            'env_batch_size': env.batch_size,\n",
    "      }\n",
    "\n",
    "      wandb.init(project='mlrl', entity='drcope', reinit=True, config=config)\n",
    "\n",
    "      for i in range(num_iterations):\n",
    "            iteration_logs = {'iteration': i}\n",
    "\n",
    "            print(f'Iteration: {i}')\n",
    "\n",
    "            # its very important to reset the actors\n",
    "            # otherwise the observations can be wrong on the next run\n",
    "            collect_actor.reset()\n",
    "            eval_actor.reset()\n",
    "            for metric in eval_actor.metrics:\n",
    "                  metric.reset()\n",
    "            for metric in collect_actor.metrics:\n",
    "                  metric.reset()\n",
    "\n",
    "            if i % eval_interval == 0:\n",
    "                  eval_actor.run_and_log()\n",
    "                  print('Evaluation stats:')\n",
    "                  print(', '.join([f'{metric.name}: {metric.result():.3f}' for metric in eval_actor.metrics]))\n",
    "                  iteration_logs.update({f'Eval{metric.name}': metric.result() for metric in eval_actor.metrics})\n",
    "\n",
    "                  try:\n",
    "                        video_file = f'{videos_dir}/video_{i}.mp4'\n",
    "                        create_policy_eval_video(agent.policy, env, max_steps=120,\n",
    "                                                 filename=video_file, max_envs_to_show=1)\n",
    "                        iteration_logs['video'] = wandb.Video(video_file, fps=30, format=\"mp4\")\n",
    "                  except Exception as e:\n",
    "                        print(f'Error creating video: {e}')\n",
    "\n",
    "            collect_actor.run()\n",
    "            print('Collect stats:')\n",
    "            print(', '.join([f'{metric.name}: {metric.result():.3f}' for metric in collect_actor.metrics]))\n",
    "            iteration_logs.update({metric.name: metric.result() for metric in collect_actor.metrics})\n",
    "\n",
    "            loss_info = ppo_learner.run()\n",
    "            print('Training info:')\n",
    "            print(f'Loss: {loss_info.loss:.5f}, '\n",
    "                  f'KL Penalty Loss: {loss_info.extra.kl_penalty_loss:.5f}, '\n",
    "                  f'Entropy: {loss_info.extra.entropy_regularization_loss:.5f}, '\n",
    "                  f'Value Estimation Loss: {loss_info.extra.value_estimation_loss:.5f}, '\n",
    "                  f'PG Loss {loss_info.extra.policy_gradient_loss:.5f}')\n",
    "\n",
    "            iteration_logs.update({\n",
    "                  'loss': loss_info.loss.numpy(), \n",
    "                  **tf.nest.map_structure(lambda x: x.numpy(), loss_info.extra._asdict())\n",
    "            })\n",
    "\n",
    "            print()\n",
    "\n",
    "            wandb.log(iteration_logs)\n",
    "            replay_buffer.clear()\n",
    "\n",
    "finally:\n",
    "      wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
