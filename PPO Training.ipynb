{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.0 (SDL 2.0.16, Python 3.8.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib confmisc.c:767:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_card_driver returned error: No such file or directory\n",
      "ALSA lib confmisc.c:392:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1246:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5220:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2642:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "ALSA lib confmisc.c:767:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_card_driver returned error: No such file or directory\n",
      "ALSA lib confmisc.c:392:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1246:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5220:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2642:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "ALSA lib confmisc.c:767:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_card_driver returned error: No such file or directory\n",
      "ALSA lib confmisc.c:392:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1246:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5220:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2642:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "ALSA lib confmisc.c:767:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_card_driver returned error: No such file or directory\n",
      "ALSA lib confmisc.c:392:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1246:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5220:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2642:(snd_pcm_open_noupdate) Unknown PCM default\n"
     ]
    }
   ],
   "source": [
    "from mlrl.experiments.train_maze_agent import (\n",
    "    create_maze_meta_env, RestrictedActionsMazeState, create_batched_tf_meta_env, \n",
    "    create_agent, create_training_run, get_maze_name\n",
    ")\n",
    "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
    "from tf_agents.environments.gym_wrapper import GymWrapper\n",
    "from tf_agents.environments.batched_py_environment import BatchedPyEnvironment\n",
    "from tf_agents.train.utils import spec_utils\n",
    "\n",
    "args = {'agent': 'ppo_agent', 'meta_time_limit': 500}\n",
    "env_batch_size = 2\n",
    "\n",
    "env = BatchedPyEnvironment([\n",
    "    GymWrapper(create_maze_meta_env(RestrictedActionsMazeState, args)) \n",
    "    for _ in range(env_batch_size)\n",
    "])\n",
    "\n",
    "eval_env = BatchedPyEnvironment([\n",
    "    GymWrapper(create_maze_meta_env(RestrictedActionsMazeState, args)) \n",
    "    for _ in range(env_batch_size)\n",
    "])\n",
    "\n",
    "env.reset()\n",
    "\n",
    "observation_tensor_spec, action_tensor_spec, time_step_tensor_spec = (\n",
    "      spec_utils.get_tensor_specs(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-04 02:20:12.863269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-04 02:20:12.877261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-04 02:20:12.877838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-04 02:20:12.878984: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-04 02:20:12.881297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-04 02:20:12.881884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-04 02:20:12.882374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-04 02:20:13.708067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-04 02:20:13.708754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-04 02:20:13.708768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-10-04 02:20:13.709184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-04 02:20:13.709270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2111 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.agents.ppo.ppo_agent import PPOAgent\n",
    "from tf_agents.networks.sequential import Sequential\n",
    "from tf_agents.networks.value_network import ValueNetwork\n",
    "from tf_agents.networks.actor_distribution_network import ActorDistributionNetwork\n",
    "from tf_agents.train.utils import train_utils\n",
    "\n",
    "from mlrl.meta.search_networks import SearchActorNetwork, SearchValueNetwork\n",
    "from mlrl.meta.search_networks import SearchTransformer, CategoricalNetwork, SearchActorLogitsNetwork\n",
    "\n",
    "\n",
    "custom_objects = {\n",
    "    'SearchActorLogitsNetwork': SearchActorLogitsNetwork,\n",
    "    'SearchTransformer': SearchTransformer\n",
    "}\n",
    "\n",
    "with tf.keras.utils.custom_object_scope(custom_objects):\n",
    "    actor_dist_net = ActorDistributionNetwork(\n",
    "        observation_tensor_spec, action_tensor_spec,\n",
    "        preprocessing_layers=SearchActorLogitsNetwork(),\n",
    "        fc_layer_params=None,\n",
    "        discrete_projection_net=lambda spec: CategoricalNetwork(spec)\n",
    "    )\n",
    "\n",
    "value_net = ValueNetwork(\n",
    "    observation_tensor_spec,\n",
    "    preprocessing_layers=SearchTransformer(3, 16, 2),\n",
    "    preprocessing_combiner=tf.keras.layers.Lambda(lambda x: tf.reduce_sum(x[0], axis=-2)),\n",
    "    batch_squash=True,\n",
    "    fc_layer_params=None\n",
    ")\n",
    "\n",
    "train_step = train_utils.create_train_step()\n",
    "\n",
    "agent = PPOAgent(\n",
    "    time_step_tensor_spec,\n",
    "    action_tensor_spec,\n",
    "    actor_net=actor_dist_net,\n",
    "    value_net=value_net,\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    train_step_counter=train_step,\n",
    "    compute_value_and_advantage_in_train=False,\n",
    "    update_normalizers_in_train=False,\n",
    "    normalize_observations=False,\n",
    "    discount_factor=0.99,\n",
    "    num_epochs=1,  # deprecated param\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=env.batch_size or 1,\n",
    "    max_length=10000\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess_seq(experience, info):\n",
    "    return agent.preprocess_sequence(experience), info\n",
    "\n",
    "\n",
    "def dataset_fn():\n",
    "    ds = replay_buffer.as_dataset(sample_batch_size=8, num_steps=32)\n",
    "    return ds.map(preprocess_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.train import actor\n",
    "from tf_agents.train import learner\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.train import triggers\n",
    "\n",
    "import os\n",
    "\n",
    "root_dir = './runs/ppo_agent/test'\n",
    "\n",
    "summary_interval = 1000\n",
    "collect_sequence_length = 2048\n",
    "policy_save_interval = 5000\n",
    "# summary_interval = 1\n",
    "# collect_sequence_length = 256\n",
    "# policy_save_interval = 2\n",
    "\n",
    "saved_model_dir = os.path.join(root_dir, learner.POLICY_SAVED_MODEL_DIR)\n",
    "collect_env_step_metric = py_metrics.EnvironmentSteps()\n",
    "learning_triggers = [\n",
    "    triggers.PolicySavedModelTrigger(\n",
    "        saved_model_dir,\n",
    "        agent,\n",
    "        train_step,\n",
    "        interval=policy_save_interval,\n",
    "        metadata_metrics={\n",
    "            triggers.ENV_STEP_METADATA_KEY: collect_env_step_metric\n",
    "        }),\n",
    "    triggers.StepPerSecondLogTrigger(train_step, interval=summary_interval),\n",
    "]\n",
    "\n",
    "collect_actor = actor.Actor(\n",
    "    env,\n",
    "    agent.collect_policy,\n",
    "    train_step,\n",
    "    steps_per_run=collect_sequence_length,\n",
    "    observers=[replay_buffer.add_batch],\n",
    "    metrics=actor.collect_metrics(buffer_size=collect_sequence_length),\n",
    "    reference_metrics=[collect_env_step_metric],\n",
    "    summary_dir=os.path.join(root_dir, learner.TRAIN_DIR),\n",
    "    summary_interval=summary_interval)\n",
    "\n",
    "collect_actor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.policies import py_tf_eager_policy\n",
    "\n",
    "eval_greedy_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
    "    agent.policy, use_tf_function=True)\n",
    "\n",
    "eval_steps=1000\n",
    "\n",
    "eval_actor = actor.Actor(\n",
    "    eval_env,\n",
    "    eval_greedy_policy,\n",
    "    train_step,\n",
    "    metrics=actor.eval_metrics(buffer_size=10),\n",
    "    reference_metrics=[collect_env_step_metric],\n",
    "    summary_dir=os.path.join(root_dir, 'eval'),\n",
    "    steps_per_run=eval_steps)\n",
    "\n",
    "eval_actor.run_and_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:458: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:458: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.train.ppo_learner import PPOLearner\n",
    "\n",
    "ppo_learner = PPOLearner(\n",
    "    root_dir,\n",
    "    train_step,\n",
    "    agent,\n",
    "    experience_dataset_fn=dataset_fn,\n",
    "    normalization_dataset_fn=dataset_fn,\n",
    "    num_samples=1, num_epochs=10,  # num samples * num epochs = num iterations per run call\n",
    "    triggers=learning_triggers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdrcope\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.3 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/drcope/mlrl/runs/1c4hdyut\" target=\"_blank\">earthy-butterfly-53</a></strong> to <a href=\"https://wandb.ai/drcope/mlrl\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Collect stats:\n",
      "NumberOfEpisodes: 26.000, EnvironmentSteps: 2048.000, AverageReturn: 0.930, AverageEpisodeLength: 73.346\n",
      "Training info:\n",
      "Loss: 5.59590, KL Penalty Loss: 0.02784, Entropy: 0.00000, Value Estimation Loss: 5.64213, PG Loss -0.07407\n",
      "Evaluation stats:\n",
      "AverageReturn: 0.900, AverageEpisodeLength: 80.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (1080, 360) to (1088, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 1\n",
      "Collect stats:\n",
      "NumberOfEpisodes: 27.000, EnvironmentSteps: 2048.000, AverageReturn: 0.920, AverageEpisodeLength: 75.704\n",
      "Training info:\n",
      "Loss: 2.46232, KL Penalty Loss: 0.02856, Entropy: 0.00000, Value Estimation Loss: 2.52418, PG Loss -0.09042\n",
      "Iteration: 2\n",
      "Collect stats:\n",
      "NumberOfEpisodes: 25.000, EnvironmentSteps: 2048.000, AverageReturn: 0.924, AverageEpisodeLength: 78.720\n",
      "Training info:\n",
      "Loss: 6.40696, KL Penalty Loss: 0.00095, Entropy: 0.00000, Value Estimation Loss: 6.47580, PG Loss -0.06979\n",
      "Iteration: 3\n",
      "Collect stats:\n",
      "NumberOfEpisodes: 28.000, EnvironmentSteps: 2048.000, AverageReturn: 0.933, AverageEpisodeLength: 71.893\n",
      "Training info:\n",
      "Loss: 2.17495, KL Penalty Loss: 0.01606, Entropy: 0.00000, Value Estimation Loss: 2.23141, PG Loss -0.07251\n",
      "Iteration: 4\n",
      "Collect stats:\n",
      "NumberOfEpisodes: 27.000, EnvironmentSteps: 2049.000, AverageReturn: 0.931, AverageEpisodeLength: 70.593\n",
      "Training info:\n",
      "Loss: 4.42586, KL Penalty Loss: 0.00567, Entropy: 0.00000, Value Estimation Loss: 4.51611, PG Loss -0.09592\n",
      "Iteration: 5\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from mlrl.utils.render_utils import create_policy_eval_video\n",
    "\n",
    "\n",
    "videos_dir = root_dir + '/videos'\n",
    "\n",
    "from pathlib import Path\n",
    "Path(videos_dir).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "\n",
    "eval_interval = 5\n",
    "num_iterations = 100\n",
    "\n",
    "try:\n",
    "      config = {\n",
    "            'eval_interval': eval_interval,\n",
    "            'num_iterations': num_iterations,\n",
    "            'summary_interval': summary_interval,\n",
    "            'collect_sequence_length': collect_sequence_length,\n",
    "            'policy_save_interval': policy_save_interval,\n",
    "            'num_samples': ppo_learner._num_samples,\n",
    "            'num_epochs': ppo_learner._num_epochs,\n",
    "            'meta_discount_factor': agent._discount_factor,\n",
    "            'max_tree_size': env.envs[0].max_tree_size,\n",
    "            'env_batch_size': env.batch_size,\n",
    "      }\n",
    "\n",
    "      wandb.init(project='mlrl', entity='drcope', reinit=True, config=config)\n",
    "\n",
    "      for i in range(num_iterations):\n",
    "            iteration_logs = {'iteration': i}\n",
    "\n",
    "            print(f'Iteration: {i}')\n",
    "            for metric in eval_actor.metrics:\n",
    "                  metric.reset()\n",
    "            for metric in collect_actor.metrics:\n",
    "                  metric.reset()\n",
    "\n",
    "            collect_actor.run()\n",
    "            print('Collect stats:')\n",
    "            print(', '.join([f'{metric.name}: {metric.result():.3f}' for metric in collect_actor.metrics]))\n",
    "            iteration_logs.update({metric.name: metric.result() for metric in collect_actor.metrics})\n",
    "\n",
    "            loss_info = ppo_learner.run()\n",
    "            print('Training info:')\n",
    "            print(f'Loss: {loss_info.loss:.5f}, '\n",
    "                  f'KL Penalty Loss: {loss_info.extra.kl_penalty_loss:.5f}, '\n",
    "                  f'Entropy: {loss_info.extra.entropy_regularization_loss:.5f}, '\n",
    "                  f'Value Estimation Loss: {loss_info.extra.value_estimation_loss:.5f}, '\n",
    "                  f'PG Loss {loss_info.extra.policy_gradient_loss:.5f}')\n",
    "\n",
    "            iteration_logs.update({\n",
    "                  'loss': loss_info.loss.numpy(), \n",
    "                  **tf.nest.map_structure(lambda x: x.numpy(), loss_info.extra._asdict())\n",
    "            })\n",
    "\n",
    "            if i % eval_interval == 0:\n",
    "                  eval_actor.run_and_log()\n",
    "                  print('Evaluation stats:')\n",
    "                  print(', '.join([f'{metric.name}: {metric.result():.3f}' for metric in eval_actor.metrics]))\n",
    "                  iteration_logs.update({f'Eval{metric.name}': metric.result() for metric in eval_actor.metrics})\n",
    "\n",
    "                  video_file = f'{videos_dir}/video_{i}.mp4'\n",
    "                  create_policy_eval_video(agent.policy, env, max_steps=120, filename=video_file, max_envs_to_show=1)\n",
    "                  iteration_logs['video'] = wandb.Video(video_file, fps=30, format=\"mp4\")\n",
    "                  print()\n",
    "\n",
    "            wandb.log(iteration_logs)\n",
    "finally:\n",
    "      wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
