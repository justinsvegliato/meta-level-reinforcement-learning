from functools import cached_property
from typing import List, Dict

from pyrsistent import inc
from rlts.meta.search_tree import SearchTree, SearchTreeNode
from rlts.utils import one_hot, compute_positional_encoding

from abc import ABC, abstractmethod

import numpy as np


class TreeTokeniser(ABC):
    """Abstract base class for tree tokenisers."""

    def __init__(self,
                 max_tokens: int,
                 one_hot_node_id: bool = False,
                 pos_enc_dim: int = 8,
                 n_terminate_tokens: int = 1):
        self.max_tokens = max_tokens  # (non-terminate) tokens
        self.one_hot_node_id = one_hot_node_id
        self.pos_enc_dim = pos_enc_dim
        self.n_tokens = self.max_tokens + n_terminate_tokens
        self.meta_vars = dict()
        self.meta_vec = np.array([])

    @abstractmethod
    def tokenise(self, tree: SearchTree) -> np.ndarray:
        """Tokenise a tree into a list of tokens."""
        pass

    @abstractmethod
    def get_token_labels(self) -> List[str]:
        """Get the labels for the tokens."""
        pass

    @cached_property
    def tree_token_dim(self) -> int:
        """Get the dimension of the tree token."""
        return len(self.get_token_labels())

    def get_terminate_token(self) -> np.ndarray:
        """Get the token for terminate action."""
        return np.array([1., 1.] + [0.] * (self.tree_token_dim - 3) + [1.])

    def set_meta_vars(self, **kwargs: Dict[str, float]):
        """Set the meta variables."""
        self.meta_vars = kwargs
        self.meta_vec = np.array([v for v in self.meta_vars.values()])

    def pad(self, tokens: np.ndarray) -> np.ndarray:
        """Pad a list of tokens to the maximum tree size."""
        if tokens.size > 0:
            if len(tokens.shape) < 2:
                raise ValueError(f'Tokens must be a 2D array. {tokens.shape=}')

            if tokens.shape[0] >= self.n_tokens:
                return tokens[:self.n_tokens]

            padding = np.zeros((self.n_tokens - tokens.shape[0], tokens.shape[1]))
            return np.concatenate([tokens, padding], axis=0)
        else:
            return np.zeros((self.n_tokens, self.tree_token_dim))

    def encode_node_id(self, node_id: int) -> np.ndarray:
        """Encode the node id as a vector."""
        if self.one_hot_node_id:
            return one_hot(node_id, self.max_tokens)
        else:
            return compute_positional_encoding(node_id, self.pos_enc_dim)

    def get_id_dim(self) -> int:
        """Get the dimension of the id vector."""
        if self.one_hot_node_id:
            return self.max_tokens
        else:
            return self.pos_enc_dim

    @abstractmethod
    def can_tokenise(self, tree: SearchTree) -> bool:
        """Check if a tree can be tokenised."""
        pass


class NodeTokeniser(TreeTokeniser):
    """Tokenise a tree into a list of vectors, with a token corresponding to each node."""

    def __init__(self,
                 max_tokens: int,
                 action_vec_dim: int,
                 state_vec_dim: int,
                 include_unexpandable_leaves: bool = True,
                 include_state_info: bool = True,
                 include_structure_info: bool = True,
                 **kwargs):
        super().__init__(max_tokens, **kwargs)
        self.action_vec_dim = action_vec_dim
        self.state_vec_dim = state_vec_dim
        self.include_unexpandable_leaves = include_unexpandable_leaves
        self.include_state_info = include_state_info
        self.include_structure_info = include_structure_info

    def node_tokenisation(self, tree: SearchTree, node: SearchTreeNode) -> np.ndarray:
        """
        Generates a token for the given node.
        This will be used when computational actions correspond to expanding a node by
        trying all possible actions, thus it does not include information about
        an action to expand the node with.

        A Token encodes the following information:
            - Attention mask. Whether the token contains valid information or is padding.
            - Can expand. Whether the node can be expanded. Used to mask out invalid actions.
            - Reward. The reward given for the antecendent action.
            - Expected root return. The expected return from the root for a trajectory passing through node.
            - Path return. The sum of rewards from the root to the node.
            - Value. The value of the node state.
            - ID: The id of the node.
            - Parent ID: The id of the parent node.
            - State vector. The state vector of the node.
            - Antecedent action vector. Vector encoding the action that was taken to get to this node.
            - Terminate Action: The action that terminates the search.
                Always zero for tokens generated by this function.

        Args:
            node: The node to tokenise
            action_idx: The index of the action to expand the node with. If this is greater than the
                number of actions the node can be expanded with, the token will be a padding token.

        Returns:
            A 1-dimensional numpy array of length token_dim.
        """
        if node.token is not None:
            # only value that can have changed
            node.token[1] = float(tree.has_valid_expansions(node))
            return node.token

        state = node.get_state()

        # node features contains a mask attention and reward/value info
        node_features = np.array([
            1.,  # mask for attention
            tree.has_valid_expansions(node),  # mask for action selection
            node.reward,  # reward to get to this node from parent state
            node.get_exp_root_return(),  # expected return from root
            node.path_return,  # sum of rewards from root to node
            node.get_value()  # value of the node state
        ], dtype=np.float32)

        feature_vecs = [node_features]

        if self.include_structure_info:

            id_vec = self.encode_node_id(node.get_id())
            if node.is_root():
                parent_id_vec = np.zeros((id_vec.size,), dtype=np.float32)
            else:
                parent_id_vec = self.encode_node_id(node.get_parent_id())
                action_taken_vec = state.get_action_vector(node.get_action())

            feature_vecs.extend([id_vec, parent_id_vec])
        
        if node.is_root():
            action_taken_vec = np.zeros((self.action_vec_dim,), dtype=np.float32)
        else:
            action_taken_vec = state.get_action_vector(node.get_action())

        feature_vecs.append(action_taken_vec)

        if self.include_state_info:
            state_vec = state.get_state_vector()
            feature_vecs.append(state_vec)

        feature_vecs.extend([
            self.meta_vec,
            np.array([0.])  # flag as not a terminate token
        ])

        node.token = np.concatenate(feature_vecs)

        return node.token

    def tokenise(self, tree: SearchTree) -> np.ndarray:
        terminate_token = self.get_terminate_token()
        tokens = np.array([terminate_token] + [
            self.node_tokenisation(tree, node)
            for node in tree.node_list
            if self.include_unexpandable_leaves or not node.is_unexpandable_leaf()
        ], dtype=np.float32)
        return self.pad(tokens)

    def get_token_labels(self) -> List[str]:
        labels = [
            'attn_mask', 'can_expand', 'reward',
            'exp_root_return', 'path_return', 'exp_value'
        ]

        if self.include_structure_info:
            id_dim = self.get_id_dim()
            id_vec = [f'id_{i}' for i in range(id_dim)]
            parent_id_vec = [f'parent_id_{i}' for i in range(id_dim)]
            labels += id_vec + parent_id_vec

        labels += [f'action_taken_{i}' for i in range(self.action_vec_dim)]

        if self.include_state_info:
            labels += [f'state_{i}' for i in range(self.state_vec_dim)]

        labels += [f'meta_{var}' for var in self.meta_vars]
        labels += [r'$\perp$']  # terminate symbol

        return labels

    def can_tokenise(self, tree: SearchTree) -> bool:
        """ Check if a tree can be tokenised."""
        n = sum([
            1 for node in tree.node_list
            if self.include_unexpandable_leaves or not node.is_unexpandable_leaf()
        ])
        return n <= self.max_tokens

    def get_node_idx(self, tree: SearchTree, computational_action: int) -> int:
        """Get the index of the node that the given computational action expands."""
        if self.include_unexpandable_leaves:
            return computational_action - 1

        # skip unexpandable leaves
        node_idx = 0
        for node in tree.node_list:
            if not node.is_unexpandable_leaf():
                if computational_action - 1 == node_idx:
                    return node.get_id()
                node_idx += 1

        raise ValueError(f'Computational action {computational_action} is not valid.')


class NodeActionTokeniser(NodeTokeniser):
    """
    Tokenise a tree into a list of vectors, with a
    token corresponding to each node-action pair.
    """

    def tokenise(self, tree: SearchTree) -> np.ndarray:
        tree_tokens = []
        # appends each action vector to each node token
        for node in tree.node_list:
            node_token = self.node_tokenisation(tree, node)
            for action in node.state.get_actions():
                action_vec = node.state.get_action_vector(action)
                token = np.concatenate([node_token, action_vec])
                tree_tokens.append(token)

        terminate_token = self.get_terminate_token()
        tokens = np.array([terminate_token] + tree_tokens, dtype=np.float32)
        return self.pad(tokens)

    def get_token_labels(self) -> List[str]:
        labels = super().get_token_labels()
        action_vec = [f'action_{i}' for i in range(self.action_vec_dim)]
        return labels + action_vec

    def can_tokenise(self, tree: SearchTree) -> bool:
        """ Check if a tree can be tokenised."""
        n = sum([
            node.state.get_maximum_number_of_actions()
            for node in tree.node_list
            if self.include_unexpandable_leaves or not node.is_unexpandable_leaf()
        ])
        return n <= self.max_tokens

    def get_node_idx(self, tree: SearchTree, computational_action: int) -> int:
        """Get the index of the node that the given computational action expands."""
        raise NotImplementedError('This tokeniser does not support this method yet.')
