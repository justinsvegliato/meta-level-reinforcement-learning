{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .support_message_main_box {\n",
       "                position: relative;\n",
       "                display: table-cell;\n",
       "                vertical-align: middle;\n",
       "                width: 100%;\n",
       "                height: 8em;\n",
       "                padding: 1em;\n",
       "                padding-left: 11em;\n",
       "                background-color: #f7f7f7;\n",
       "                border: 1px solid #cfcfcf;\n",
       "                border-radius: 2px;\n",
       "            }\n",
       "            .support_message_main_box img {\n",
       "                position: absolute;\n",
       "                height: 9em;\n",
       "                width: 9em;\n",
       "                left: 0.5em;\n",
       "                top: 0.5em;\n",
       "                border-radius: 1em;\n",
       "            }\n",
       "        </style>\n",
       "        <div class=\"support_message_main_box\">\n",
       "            <img src=\"https://avatars.githubusercontent.com/u/7738570?v=4\" />\n",
       "            <p>\n",
       "            <b>Hi!</b><br/>\n",
       "            <span>I am the author of\n",
       "            <a href=\"https://github.com/LucaCappelletti94/silence_tensorflow\" target=\"_blank\">\n",
       "                silence_tensorflow\n",
       "            </a>, which you use in this Notebook.\n",
       "            </span><br/>\n",
       "            \n",
       "            <span>I love to code, but I also need coffee.</span>\n",
       "            <a href=\"https://github.com/sponsors/LucaCappelletti94\" target=\"_blank\">\n",
       "                Please sponsor me on GitHub ‚ù§Ô∏è\n",
       "            </a><br/>\n",
       "            <i>Good luck in your coding üçÄ!</i>\n",
       "            <br/>\n",
       "            <i>- Luca</i>\n",
       "            </p>\n",
       "        <div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.0 (SDL 2.0.16, Python 3.8.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from mlrl.runners.dqn_runner import DQNRun\n",
    "from mlrl.experiments.procgen_dqn import make_procgen, create_rainbow_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'procgen_env_name': 'coinrun',\n",
    "    'frame_stack': 4,\n",
    "    'grayscale': False,\n",
    "    'agent': 'rainbow',\n",
    "    'learning_rate': 2.5e-4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = config.get('procgen_env_name', 'coinrun')\n",
    "env = make_procgen(env_name, config)\n",
    "# q_net, agent = create_rainbow_agent(env, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.agents.dqn.dqn_agent import DdqnAgent\n",
    "from tf_agents.networks.q_network import QNetwork\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.networks.categorical_q_network import CategoricalQNetwork\n",
    "from tf_agents.agents import CategoricalDqnAgent\n",
    "from tf_agents.environments.py_environment import PyEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_q_net = CategoricalQNetwork(\n",
    "#     tensor_spec.from_spec(env.observation_spec()),\n",
    "#     tensor_spec.from_spec(env.action_spec()),\n",
    "#     conv_layer_params=[(64, 8, 4), (64, 4, 2), (64, 3, 2)],\n",
    "#     fc_layer_params=[512]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RainbowQNet(tf.keras.Sequential):\n",
    "\n",
    "#     def __init__(self, observation_shape, n_actions, name='RainbowQNet'):\n",
    "#         super(RainbowQNet, self).__init__(name=name)\n",
    "\n",
    "#         self.observation_shape = observation_shape\n",
    "#         self.n_actions = n_actions\n",
    "\n",
    "#         self.network = CategoricalQNetwork(\n",
    "#             tensor_spec.TensorSpec(shape=observation_shape, dtype=tf.float64, name='observation'),\n",
    "#             tensor_spec.BoundedTensorSpec(shape=(), dtype=tf.int64, minimum=0, maximum=n_actions - 1, name='action'),\n",
    "#             conv_layer_params=[(64, 8, 4), (64, 4, 2), (64, 3, 2)],\n",
    "#             fc_layer_params=[512]\n",
    "#         )\n",
    "\n",
    "#         self.add(self.network)\n",
    "\n",
    "#         self(tf.keras.Input(shape=(1,) + tuple(observation_shape), dtype=tf.float64))\n",
    "    \n",
    "#     def get_config(self):\n",
    "#         return {\n",
    "#             'observation_shape': self.observation_shape,\n",
    "#             'n_actions': self.n_actions\n",
    "#         }\n",
    "    \n",
    "#     @classmethod\n",
    "#     def from_config(cls, config):\n",
    "#         return cls(**config)\n",
    "    \n",
    "#     def create_agent(self, env, config):\n",
    "#         return CategoricalDqnAgent(\n",
    "#             env.time_step_spec(),\n",
    "#             env.action_spec(),\n",
    "#             q_network=self,\n",
    "#             optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "#             td_errors_loss_fn=tf.math.squared_difference,\n",
    "#             gamma=0.99,\n",
    "#             train_step_counter=tf.Variable(0)\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrl.networks.cnn_q_nets import RainbowQNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_shape = env.observation_spec().shape\n",
    "n_actions = env.action_spec().num_values\n",
    "learning_rate = config.get('learning_rate', 2.5e-4)\n",
    "\n",
    "rainbow_q_net = RainbowQNet(observation_shape, n_actions, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrl.procgen import REWARD_BOUNDS as PROCGEN_REWARD_BOUNDS\n",
    "\n",
    "if env_name not in PROCGEN_REWARD_BOUNDS:\n",
    "    raise ValueError(f'Unknown reward bounds for procgen env: {env_name}')\n",
    "r_min, r_max = PROCGEN_REWARD_BOUNDS[env_name]\n",
    "\n",
    "config['r_min'] = r_min\n",
    "config['r_max'] = r_max\n",
    "\n",
    "rainbow_agent = rainbow_q_net.create_agent(env, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrl.experiments.procgen_dqn import create_video_renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_video = create_video_renderer(env_name, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'video.mp4'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.policies.py_tf_eager_policy import PyTFEagerPolicy\n",
    "render_video(PyTFEagerPolicy(rainbow_agent.collect_policy, use_tf_function=True, batch_time_steps=False), 'video.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainbow_q_net(env.current_time_step().observation);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainbow_q_net.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainbow_reloaded = tf.keras.models.load_model('model.h5', custom_objects={'RainbowQNet': RainbowQNet})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 15, 51])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rainbow_reloaded(env.current_time_step().observation)[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
